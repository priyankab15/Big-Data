{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Spark (15 points)\n",
    "\n",
    "**Read through the entire workbook before beginning work to familiarize with what is being asked.**\n",
    "\n",
    "In this exericise, you will work with a fairly large dataset and do some modeling and predictions using Spark Machine Learning. The dataset being used is the 2013 NYC Taxi Dataset [https://archive.org/details/nycTaxiTripData2013](https://archive.org/details/nycTaxiTripData2013). The original dataset consists of two files, `trip_data` and `trip_fare`, which need to be merged to create the dataset for modeling. \n",
    "\n",
    "The merge has already been performed and stored as a parquet file on S3 because there were some data issues that were not covered in class. The workbook [merge-fare-and-trip-data-create-parquet.ipynb](merge-fare-and-trip-data-create-parquet.ipynb) contains code for creating the merged set (for reference.)\n",
    "\n",
    "This is an interactive PySpark session. Remember that when you open this notebook the SparkContext and SparkSession are already created, and they are in the sc and spark variables, respectively. You can run the following two cells to make sure that the Kernel is active.\n",
    "\n",
    "**Do not insert any additional cells than the ones that are provided for assignment submission. You may add cells as you work through the notebook, but you need to delete extra cells and keep the initial structure.**\n",
    "\n",
    "### Note on time it takes to complete the model training\n",
    "\n",
    "When we designed the assignment, it was thoroughly tested and the training time took no more than 30-40 mins with the provided configuration on the entire dataset. In previous courses, several students told us that the training is taking too long, more than that. There is no direct way to troubleshoot. That is one of the unfortunate realities with working with these tools.\n",
    "\n",
    "If you do run into problems, you have two options:\n",
    "\n",
    "- Try adding more core or task nodes to the cluster, and restart your notebook and try again.\n",
    "- Your other option is  to take a 10% sample of the \"encoded_dataset\" before you split into 90% training and 10% testing. \n",
    "\n",
    "### Some suggestions you may want to follow:\n",
    "\n",
    "* You may consider saving intermediate datasets (i.e. training and testing) when you first create them. To save intermediate datasets, save them as **parquet** files in your own S3 bucket. To save, use the following code: `df.write.parquet(\"s3://[[your-s3-bucket]]/data_location/\", mode=\"overwrite\")`.\n",
    "* You may also want to save a model object in S3 after you train it, especially if training takes a while. To save a model object, use the following code: `model.save(\"s3://[[your-s3-bucket]]/model_location/\")`\n",
    "* When creating the Machine Learning pipelines, you may want to try it first on a minuscule sample of your training data to make sure the pipelines work as planned. To create a tiny DataFrame, use the `limit` method: `df.limit(100)` (this creates a small DataFrame with the first 100 rows from df.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-87-255.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-87-255.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f3e0a511a10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries\n",
    "\n",
    "The following cell will load all required libraries and functions for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer, VectorAssembler\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.ml.feature import RFormula\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Load the data, convert data types and create new features (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n",
    "In the following cell, create an object called `nyctaxi` which loads the **parquet** files from `s3://bigdatateaching/nyctaxi-2013/merged-parquet/`.\n",
    "\n",
    "This step took approximately 8 seconds during the development of the workbook using the cluster configuration in the assignment. The workbook will have spots were running times during development are written for reference as a comment in a cell. Your run times may vary, but these values will give you an indication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~ 8 seconds\n",
    "nyctaxi= sqlContext.read.parquet('s3://bigdatateaching/nyctaxi-2013/merged-parquet/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell print the schema of `nyctaxi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- medallion: string (nullable = true)\n",
      " |-- hack_license: string (nullable = true)\n",
      " |-- vendor_id: string (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- surcharge: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- total: string (nullable = true)\n",
      " |-- rate_code: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- trip_time_in_secs: string (nullable = true)\n",
      " |-- trip_distance: string (nullable = true)\n",
      " |-- pickup_longitude: string (nullable = true)\n",
      " |-- pickup_latitude: string (nullable = true)\n",
      " |-- dropoff_longitude: string (nullable = true)\n",
      " |-- dropoff_latitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nyctaxi.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, count the number of records for `nyctaxi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 173185091\n"
     ]
    }
   ],
   "source": [
    "# ~ 12 seconds\n",
    "print(\"Number of records: \" + str(nyctaxi.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, show the first 10 records of `nyctaxi` to see what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+-------------------+------------+-----------+---------+-------+----------+------------+-----+---------+------------------+-------------------+---------------+-----------------+-------------+----------------+---------------+-----------------+----------------+\n",
      "|           medallion|        hack_license|vendor_id|    pickup_datetime|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|total|rate_code|store_and_fwd_flag|   dropoff_datetime|passenger_count|trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|\n",
      "+--------------------+--------------------+---------+-------------------+------------+-----------+---------+-------+----------+------------+-----+---------+------------------+-------------------+---------------+-----------------+-------------+----------------+---------------+-----------------+----------------+\n",
      "|00005007A9F30E289...|132A7AC13C8471488...|      CMT|2013-07-30 22:50:31|         CRD|       10.5|      0.5|    0.5|         3|           0| 14.5|        1|                 N|2013-07-30 23:01:50|              1|              679|         2.50|      -73.981644|      40.752388|       -73.982964|       40.775761|\n",
      "|00005007A9F30E289...|16780B3E72BAA7A5C...|      CMT|2013-07-16 19:22:49|         CSH|        2.5|        1|    0.5|         0|           0|    4|        1|                 N|2013-07-16 19:40:47|              1|             1078|          .00|       -73.98587|      40.759899|       -73.985451|       40.747894|\n",
      "|00005007A9F30E289...|24C122A944FB8EE21...|      CMT|2013-10-11 11:18:25|         CRD|        5.5|        0|    0.5|         1|           0|    7|        1|                 Y|2013-10-11 11:23:11|              1|              285|          .90|      -73.997002|       40.72245|       -73.986115|       40.729259|\n",
      "|00005007A9F30E289...|24C122A944FB8EE21...|      CMT|2013-10-21 07:45:29|         CRD|        7.5|        0|    0.5|       1.1|           0|  9.1|        1|                 N|2013-10-21 07:52:41|              1|              432|         1.70|      -73.958084|      40.778934|       -73.975479|       40.760258|\n",
      "|00005007A9F30E289...|24C122A944FB8EE21...|      CMT|2013-10-21 16:23:04|         CSH|        5.5|        1|    0.5|         0|           0|    7|        1|                 N|2013-10-21 16:28:01|              3|              297|          .90|       -73.97213|      40.793816|       -73.962753|       40.804379|\n",
      "|00005007A9F30E289...|24C122A944FB8EE21...|      CMT|2013-10-23 06:40:22|         CRD|         10|        0|    0.5|       2.1|           0| 12.6|        1|                 N|2013-10-23 06:48:34|              1|              491|         2.80|      -73.973419|      40.784752|       -73.982384|       40.755123|\n",
      "|00005007A9F30E289...|24C122A944FB8EE21...|      CMT|2013-10-30 09:52:19|         CRD|          6|        0|    0.5|       1.3|           0|  7.8|        1|                 N|2013-10-30 09:59:08|              1|              409|          .70|      -73.980362|      40.751549|       -73.985512|       40.743267|\n",
      "|00005007A9F30E289...|24C122A944FB8EE21...|      CMT|2013-10-30 14:32:32|         CSH|        7.5|        0|    0.5|         0|           0|    8|        1|                 N|2013-10-30 14:41:54|              1|              561|          .90|       -73.98616|      40.755646|       -73.969231|       40.761013|\n",
      "|00005007A9F30E289...|24C122A944FB8EE21...|      CMT|2013-11-07 07:40:09|         CSH|        2.5|        0|    0.5|         0|           0|    3|        1|                 N|2013-11-07 07:45:16|              1|              307|          .00|      -73.956886|       40.77837|       -73.970078|       40.764481|\n",
      "|00005007A9F30E289...|24C122A944FB8EE21...|      CMT|2013-11-07 10:26:14|         CRD|          8|        0|    0.5|         1|           0|  9.5|        1|                 N|2013-11-07 10:37:14|              1|              660|         1.00|       -74.00441|      40.752228|       -73.992271|       40.755032|\n",
      "+--------------------+--------------------+---------+-------------------+------------+-----------+---------+-------+----------+------------+-----+---------+------------------+-------------------+---------------+-----------------+-------------+----------------+---------------+-----------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ~ 4 seconds\n",
    "nyctaxi.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data types\n",
    "\n",
    "As you can see from printing the `nyctaxi` schema, all of the fields were loaded as strings. This will not work for modeling purposes. You need to convert some of the fields to other types. In the following cell, create a new DataFrame called `nyctaxi_converted` and use the `withColumn` method to do the following conversions:\n",
    "\n",
    " Field Name | Type\n",
    "------------|-----\n",
    "trip_time_in_seconds | integer\n",
    "trip_distance | float\n",
    "pickup_latitude | float\n",
    "pickup_longitude | float\n",
    "dropoff_latitude | float\n",
    "dropoff_longitude | float\n",
    "fare_amount | float\n",
    "surcharge | float\n",
    "mta_tax | float\n",
    "tip_amount | float\n",
    "tolls_amount | float\n",
    "total | float\n",
    "pickup_datetime | timestamp\n",
    "dropoff_datetime | timestamp\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyctaxi_converted= nyctaxi.withColumn(\"trip_time_in_secs\", nyctaxi.trip_time_in_secs.cast('Integer'))\\\n",
    "                .withColumn(\"trip_distance\", nyctaxi.trip_distance.cast('float'))\\\n",
    "                .withColumn(\"pickup_latitude\", nyctaxi.pickup_latitude.cast('float'))\\\n",
    "                .withColumn(\"pickup_longitude\", nyctaxi.pickup_longitude.cast('float'))\\\n",
    "                .withColumn(\"dropoff_latitude\", nyctaxi.dropoff_latitude.cast('float'))\\\n",
    "                .withColumn(\"dropoff_longitude\", nyctaxi.dropoff_longitude.cast('float'))\\\n",
    "                .withColumn(\"fare_amount\", nyctaxi.fare_amount.cast('float'))\\\n",
    "                .withColumn(\"surcharge\", nyctaxi.surcharge.cast('float'))\\\n",
    "                .withColumn(\"mta_tax\", nyctaxi.mta_tax.cast('float'))\\\n",
    "                .withColumn(\"tip_amount\", nyctaxi.tip_amount.cast('float'))\\\n",
    "                .withColumn(\"tolls_amount\", nyctaxi.tolls_amount.cast('float'))\\\n",
    "                .withColumn(\"total\", nyctaxi.total.cast('float'))\\\n",
    "                .withColumn(\"pickup_datetime\", nyctaxi.pickup_datetime.cast('timestamp'))\\\n",
    "                .withColumn(\"dropoff_datetime\", nyctaxi.dropoff_datetime.cast('timestamp'))\\\n",
    "                .withColumn(\"passenger_count\", nyctaxi.passenger_count.cast('Integer'))\n",
    "\n",
    "# drop_list= ['vendor_id','medallion','hack_license','payment_type','rate_code','store_and_fwd_flag',\n",
    "#             'passenger_count','trip_time_in_secs']\n",
    "# nyctaxi_converted= nyctaxi.select([column for column in nyctaxi_converted.columns if column not in drop_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- medallion: string (nullable = true)\n",
      " |-- hack_license: string (nullable = true)\n",
      " |-- vendor_id: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: float (nullable = true)\n",
      " |-- surcharge: float (nullable = true)\n",
      " |-- mta_tax: float (nullable = true)\n",
      " |-- tip_amount: float (nullable = true)\n",
      " |-- tolls_amount: float (nullable = true)\n",
      " |-- total: float (nullable = true)\n",
      " |-- rate_code: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_time_in_secs: integer (nullable = true)\n",
      " |-- trip_distance: float (nullable = true)\n",
      " |-- pickup_longitude: float (nullable = true)\n",
      " |-- pickup_latitude: float (nullable = true)\n",
      " |-- dropoff_longitude: float (nullable = true)\n",
      " |-- dropoff_latitude: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nyctaxi_converted.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will add a few new fields with data derived from current fields as new features.\n",
    "\n",
    "In the following cell, add the following columns to `nyctaxi_converted` using the `withColumn` method:\n",
    "\n",
    "* A column called `pickup_hour` with the hour from `pickup_datetime`. This provides an integer from 0 to 23.\n",
    "* A column called `pickup_week` with the week of the year from `pickup_datetime`. This provides an integer from 1 to 53.\n",
    "* A column called `weekday` with the name of the day of the week ffrom `pickup_datetime`, in long format.\n",
    "* A column called `tipped` which is an indicator of wether or not there was a tip. If the tip is 0, then it's 0, otherwise 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nyctaxi_converted.select('pickup_datetime').show(5)\n",
    "def valueToCategory(value):\n",
    "    if   value == 0: return 0\n",
    "    else: return 1\n",
    "udfValueToCategory = udf(valueToCategory, IntegerType())\n",
    "\n",
    "nyctaxi_converted=nyctaxi_converted.withColumn('pickup_hour', hour(col(\"pickup_datetime\")))\\\n",
    "                 .withColumn(\"pickup_week\",weekofyear(col(\"pickup_datetime\")))\\\n",
    "                 .withColumn(\"weekday\", date_format('pickup_datetime', 'E').alias('dow_string'))\\\n",
    "                 .withColumn(\"tipped\", udfValueToCategory(\"tip_amount\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the creation of the next feature, you will use a SparkSQL statement. Therefore, you need to register your DataFrame. In the next cell, register the `nyctaxi_converted` DataFrame as `nyctaxi_converted_tbl.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyctaxi_converted.createOrReplaceTempView(\"nyctaxi_converted_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, you will add a new column called `time_bins` that takes the value of `pickup_hour` and buckets it according to the following rules. You must assign the SQL statement back to `nyctaxi_converted`:\n",
    "\n",
    "* If the value of the pickup hour is at-or-before 6am, or at-or-after 8pm, then the value is \"night\"\n",
    "* If the value of the pickup hour is between 7am and 10am (inclusive), then the value is \"am_rush\"\n",
    "* If the value of the pickup hour is between 11am and 3pm (inclusive), then the value is \"afternoon\"\n",
    "* If the value of the pickup hour is between 4pm and 7pm (inclusive), then the value is \"pm_rush\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timebins(time):\n",
    "    if time<=6 and time>=20: return \"night\"\n",
    "    elif time>7 and time<=10: return \"am_rush\"\n",
    "    elif time>11 and time<=13: return \"afternoon\"\n",
    "    elif time>16 and time<=19: return \"pm_rush\"\n",
    "    else: return \"non_peak\"\n",
    "    \n",
    "udftimebins = udf(timebins, StringType())\n",
    "nyctaxi_converted= nyctaxi_converted.withColumn('time_bins',udftimebins('pickup_hour') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, print the schema of your `nyctaxi_converted` DataFrame and make sure it compares to the results below. Field order is not important.\n",
    "\n",
    "```\n",
    "root\n",
    " |-- medallion: string (nullable = true)\n",
    " |-- hack_license: string (nullable = true)\n",
    " |-- vendor_id: string (nullable = true)\n",
    " |-- pickup_datetime: timestamp (nullable = true)\n",
    " |-- payment_type: string (nullable = true)\n",
    " |-- fare_amount: float (nullable = true)\n",
    " |-- surcharge: float (nullable = true)\n",
    " |-- mta_tax: float (nullable = true)\n",
    " |-- tip_amount: float (nullable = true)\n",
    " |-- tolls_amount: float (nullable = true)\n",
    " |-- total: float (nullable = true)\n",
    " |-- rate_code: string (nullable = true)\n",
    " |-- store_and_fwd_flag: string (nullable = true)\n",
    " |-- dropoff_datetime: timestamp (nullable = true)\n",
    " |-- passenger_count: integer (nullable = true)\n",
    " |-- trip_time_in_secs: float (nullable = true)\n",
    " |-- trip_distance: float (nullable = true)\n",
    " |-- pickup_longitude: float (nullable = true)\n",
    " |-- pickup_latitude: float (nullable = true)\n",
    " |-- dropoff_longitude: float (nullable = true)\n",
    " |-- dropoff_latitude: float (nullable = true)\n",
    " |-- pickup_hour: integer (nullable = true)\n",
    " |-- pickup_week: integer (nullable = true)\n",
    " |-- weekday: string (nullable = true)\n",
    " |-- tipped: integer (nullable = false)\n",
    " |-- time_bins: string (nullable = true)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- medallion: string (nullable = true)\n",
      " |-- hack_license: string (nullable = true)\n",
      " |-- vendor_id: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: float (nullable = true)\n",
      " |-- surcharge: float (nullable = true)\n",
      " |-- mta_tax: float (nullable = true)\n",
      " |-- tip_amount: float (nullable = true)\n",
      " |-- tolls_amount: float (nullable = true)\n",
      " |-- total: float (nullable = true)\n",
      " |-- rate_code: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_time_in_secs: integer (nullable = true)\n",
      " |-- trip_distance: float (nullable = true)\n",
      " |-- pickup_longitude: float (nullable = true)\n",
      " |-- pickup_latitude: float (nullable = true)\n",
      " |-- dropoff_longitude: float (nullable = true)\n",
      " |-- dropoff_latitude: float (nullable = true)\n",
      " |-- pickup_hour: integer (nullable = true)\n",
      " |-- pickup_week: integer (nullable = true)\n",
      " |-- weekday: string (nullable = true)\n",
      " |-- tipped: integer (nullable = true)\n",
      " |-- time_bins: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nyctaxi_converted.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, show the first 10 rows of `nyctaxi_converted` to see if your new fields are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+-------------------+------------+-----------+---------+-------+----------+------------+-----+---------+------------------+-------------------+---------------+-----------------+-------------+----------------+---------------+-----------------+----------------+-----------+-----------+-------+------+---------+\n",
      "|           medallion|        hack_license|vendor_id|    pickup_datetime|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|total|rate_code|store_and_fwd_flag|   dropoff_datetime|passenger_count|trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|pickup_hour|pickup_week|weekday|tipped|time_bins|\n",
      "+--------------------+--------------------+---------+-------------------+------------+-----------+---------+-------+----------+------------+-----+---------+------------------+-------------------+---------------+-----------------+-------------+----------------+---------------+-----------------+----------------+-----------+-----------+-------+------+---------+\n",
      "|00005007A9F30E289...|132A7AC13C8471488...|      CMT|2013-07-30 22:50:31|         CRD|       10.5|      0.5|    0.5|       3.0|         0.0| 14.5|        1|                 N|2013-07-30 23:01:50|              1|              679|          2.5|       -73.98164|      40.752388|        -73.98296|        40.77576|         22|         31|    Tue|     1| non_peak|\n",
      "|00005007A9F30E289...|16780B3E72BAA7A5C...|      CMT|2013-07-16 19:22:49|         CSH|        2.5|      1.0|    0.5|       0.0|         0.0|  4.0|        1|                 N|2013-07-16 19:40:47|              1|             1078|          0.0|       -73.98587|        40.7599|        -73.98545|       40.747894|         19|         29|    Tue|     0|  pm_rush|\n",
      "|00005007A9F30E289...|24C122A944FB8EE21...|      CMT|2013-10-11 11:18:25|         CRD|        5.5|      0.0|    0.5|       1.0|         0.0|  7.0|        1|                 Y|2013-10-11 11:23:11|              1|              285|          0.9|         -73.997|       40.72245|       -73.986115|        40.72926|         11|         41|    Fri|     1| non_peak|\n",
      "|00005007A9F30E289...|24C122A944FB8EE21...|      CMT|2013-10-21 07:45:29|         CRD|        7.5|      0.0|    0.5|       1.1|         0.0|  9.1|        1|                 N|2013-10-21 07:52:41|              1|              432|          1.7|      -73.958084|      40.778934|        -73.97548|       40.760258|          7|         43|    Mon|     1| non_peak|\n",
      "|00005007A9F30E289...|24C122A944FB8EE21...|      CMT|2013-10-21 16:23:04|         CSH|        5.5|      1.0|    0.5|       0.0|         0.0|  7.0|        1|                 N|2013-10-21 16:28:01|              3|              297|          0.9|       -73.97213|      40.793816|        -73.96275|        40.80438|         16|         43|    Mon|     0| non_peak|\n",
      "|00005007A9F30E289...|24C122A944FB8EE21...|      CMT|2013-10-23 06:40:22|         CRD|       10.0|      0.0|    0.5|       2.1|         0.0| 12.6|        1|                 N|2013-10-23 06:48:34|              1|              491|          2.8|       -73.97342|       40.78475|        -73.98238|       40.755123|          6|         43|    Wed|     1| non_peak|\n",
      "|00005007A9F30E289...|24C122A944FB8EE21...|      CMT|2013-10-30 09:52:19|         CRD|        6.0|      0.0|    0.5|       1.3|         0.0|  7.8|        1|                 N|2013-10-30 09:59:08|              1|              409|          0.7|       -73.98036|       40.75155|        -73.98551|       40.743267|          9|         44|    Wed|     1|  am_rush|\n",
      "|00005007A9F30E289...|24C122A944FB8EE21...|      CMT|2013-10-30 14:32:32|         CSH|        7.5|      0.0|    0.5|       0.0|         0.0|  8.0|        1|                 N|2013-10-30 14:41:54|              1|              561|          0.9|       -73.98616|      40.755646|        -73.96923|       40.761013|         14|         44|    Wed|     0| non_peak|\n",
      "|00005007A9F30E289...|24C122A944FB8EE21...|      CMT|2013-11-07 07:40:09|         CSH|        2.5|      0.0|    0.5|       0.0|         0.0|  3.0|        1|                 N|2013-11-07 07:45:16|              1|              307|          0.0|       -73.95689|       40.77837|        -73.97008|        40.76448|          7|         45|    Thu|     0| non_peak|\n",
      "|00005007A9F30E289...|24C122A944FB8EE21...|      CMT|2013-11-07 10:26:14|         CRD|        8.0|      0.0|    0.5|       1.0|         0.0|  9.5|        1|                 N|2013-11-07 10:37:14|              1|              660|          1.0|       -74.00441|      40.752228|        -73.99227|        40.75503|         10|         45|    Thu|     1|  am_rush|\n",
      "+--------------------+--------------------+---------+-------------------+------------+-----------+---------+-------+----------+------------+-----+---------+------------------+-------------------+---------------+-----------------+-------------+----------------+---------------+-----------------+----------------+-----------+-----------+-------+------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ~ 12 seconds\n",
    "nyctaxi_converted.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Drop unused variables and filter data for clean trips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, create a new DataFrame called `valid_trips` with the following structure. You are welcome to use DataFrame methods or SparkSQL for this.\n",
    "\n",
    "* Keep only these fields: time_bins, tipped, weekday, pickup_week, pickup_hour, trip_distance, trip_time_in_secs, passenger_count, rate_code, total, tip_amount, fare_amount, payment_type, vendor_id\n",
    "* Filter records on the following criteria:\n",
    "  * Passenger count is greater than 0 and less than 8\n",
    "  * Payment type is cash (CSH) or credit card (CRD)\n",
    "  * Tip amount is 0 or more, but less than 30\n",
    "  * Fare amount is between \\\\$1 or more, and less than \\\\$150\n",
    "  * Trip distance is less than 100 miles, but more than 0\n",
    "  * The time of the trip is 30 seconds or more, and less than 2 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_columns= ['time_bins','tipped','weekday','pickup_week','pickup_hour','trip_distance','trip_time_in_secs',\n",
    "               'passenger_count','rate_code','total','tip_amount','fare_amount','payment_type','vendor_id']\n",
    "\n",
    "valid_trips= nyctaxi_converted.select([column for column in nyctaxi_converted.columns if column in keep_columns])\n",
    "valid_trips= valid_trips.filter((valid_trips.passenger_count >0) & (valid_trips.passenger_count<8))\\\n",
    "                        .filter((valid_trips.payment_type == 'CSH') | (valid_trips.payment_type== 'CRD'))\\\n",
    "                        .filter((valid_trips.tip_amount >= 0) & (valid_trips.tip_amount < 30))\\\n",
    "                        .filter((valid_trips.fare_amount>= 1) & (valid_trips.fare_amount< 150))\\\n",
    "                        .filter((valid_trips.trip_distance> 0) & (valid_trips.trip_distance< 100))\\\n",
    "                        .filter((valid_trips.trip_time_in_secs >= 30) & (valid_trips.trip_time_in_secs< 7200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, print the schema of the `valid_trips` DataFrame and compare to this:\n",
    "\n",
    "```\n",
    "root\n",
    " |-- vendor_id: string (nullable = true)\n",
    " |-- payment_type: string (nullable = true)\n",
    " |-- fare_amount: float (nullable = true)\n",
    " |-- tip_amount: float (nullable = true)\n",
    " |-- total: float (nullable = true)\n",
    " |-- rate_code: string (nullable = true)\n",
    " |-- passenger_count: integer (nullable = true)\n",
    " |-- trip_time_in_secs: float (nullable = true)\n",
    " |-- trip_distance: float (nullable = true)\n",
    " |-- pickup_hour: integer (nullable = true)\n",
    " |-- pickup_week: integer (nullable = true)\n",
    " |-- weekday: string (nullable = true)\n",
    " |-- tipped: integer (nullable = false)\n",
    " |-- time_bins: string (nullable = true)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- vendor_id: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: float (nullable = true)\n",
      " |-- tip_amount: float (nullable = true)\n",
      " |-- total: float (nullable = true)\n",
      " |-- rate_code: string (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_time_in_secs: integer (nullable = true)\n",
      " |-- trip_distance: float (nullable = true)\n",
      " |-- pickup_hour: integer (nullable = true)\n",
      " |-- pickup_week: integer (nullable = true)\n",
      " |-- weekday: string (nullable = true)\n",
      " |-- tipped: integer (nullable = true)\n",
      " |-- time_bins: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "valid_trips.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, count the number of records for `valid_trips`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 171182914\n"
     ]
    }
   ],
   "source": [
    "# ~ 48 seconds\n",
    "print(\"Number of records: \" + str(valid_trips.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, calculate the number of records that were dropped with the filter applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records dropped: 2002177\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of records dropped: \" + str(nyctaxi.count()- valid_trips.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Feature transformation and data preparation for modeling\n",
    "\n",
    "In this section, you will convert the categorical variables `vendor_id`, `rate_code`, `payment_type`, and `time_bins` to features that can be used in the model. You need to use the `StringIndexer` function on each of the categorical variables to convert the raw text into indices, and then for each set of indices you need to use the `OneHotEncoder` function to convert the index to a vector of dummy variables.\n",
    "\n",
    "In the next 8 cells, create four `StringIndexer` objects named `si_1` through `si_4`, and four `OneHotEncoder` objects named `en_1` through `en_4` for each of the categorical variables. Each `en_` must use as input, the output from the `si_`. The four output columns for `en_` must be named `vendor_vec`, `rate_vec`, `payment_vec`, `time_bins_vec`.\n",
    "\n",
    "Make sure that you use the parameter `dropLast=False` for `OneHotEncoder`, and that the `en_` and `si_` of the same number are used on the same variable.\n",
    "\n",
    "We talked about transformers and estimators in class. Refer to the book for additional information. As a brief summary:\n",
    "* Transformers: take data in, and produce new data\n",
    "* Estimators: take data in and produce a transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "si_1 = StringIndexer(inputCol= 'vendor_id', outputCol= 'vendor_index' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_1 = OneHotEncoder(dropLast=False, inputCol = 'vendor_index', outputCol= 'vendor_vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "si_2 = StringIndexer(inputCol= 'rate_code', outputCol= 'rate_in' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_2 = OneHotEncoder(dropLast=False, inputCol = 'rate_in', outputCol= 'rate_vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "si_3 = StringIndexer(inputCol= 'payment_type', outputCol= 'payment_index' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_3 = OneHotEncoder(dropLast=False, inputCol = 'payment_index', outputCol= 'payment_vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "si_4 = StringIndexer(inputCol= 'time_bins', outputCol= 'time_bins_index' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_4 = OneHotEncoder(dropLast=False, inputCol = 'time_bins_index', outputCol= 'time_bins_vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, build a pipeline called `encoded_final` where you will run the stages in the following order: si_1, en_1, si_2, en_2, etc. You need to run a fit method and a transform method on the `vaild_trips` DataFrame in order to get the desired results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~ 8 min\n",
    "pipeline = Pipeline(stages= [si_1, en_1, si_2, en_2, si_3, en_3, si_4, en_4])\n",
    "model1= pipeline.fit(valid_trips)\n",
    "\n",
    "encoded_final= model1.transform(valid_trips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, show the first 10 rows of `encoded_final`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+-----------+----------+-----+---------+---------------+-----------------+-------------+-----------+-----------+-------+------+---------+------------+-------------+-------+--------------+-------------+-------------+---------------+-------------+\n",
      "|vendor_id|payment_type|fare_amount|tip_amount|total|rate_code|passenger_count|trip_time_in_secs|trip_distance|pickup_hour|pickup_week|weekday|tipped|time_bins|vendor_index|   vendor_vec|rate_in|      rate_vec|payment_index|  payment_vec|time_bins_index|time_bins_vec|\n",
      "+---------+------------+-----------+----------+-----+---------+---------------+-----------------+-------------+-----------+-----------+-------+------+---------+------------+-------------+-------+--------------+-------------+-------------+---------------+-------------+\n",
      "|      CMT|         CRD|       10.5|       3.0| 14.5|        1|              1|              679|          2.5|         22|         31|    Tue|     1| non_peak|         1.0|(2,[1],[1.0])|    0.0|(20,[0],[1.0])|          0.0|(2,[0],[1.0])|            0.0|(4,[0],[1.0])|\n",
      "|      CMT|         CRD|        5.5|       1.0|  7.0|        1|              1|              285|          0.9|         11|         41|    Fri|     1| non_peak|         1.0|(2,[1],[1.0])|    0.0|(20,[0],[1.0])|          0.0|(2,[0],[1.0])|            0.0|(4,[0],[1.0])|\n",
      "|      CMT|         CRD|        7.5|       1.1|  9.1|        1|              1|              432|          1.7|          7|         43|    Mon|     1| non_peak|         1.0|(2,[1],[1.0])|    0.0|(20,[0],[1.0])|          0.0|(2,[0],[1.0])|            0.0|(4,[0],[1.0])|\n",
      "|      CMT|         CSH|        5.5|       0.0|  7.0|        1|              3|              297|          0.9|         16|         43|    Mon|     0| non_peak|         1.0|(2,[1],[1.0])|    0.0|(20,[0],[1.0])|          1.0|(2,[1],[1.0])|            0.0|(4,[0],[1.0])|\n",
      "|      CMT|         CRD|       10.0|       2.1| 12.6|        1|              1|              491|          2.8|          6|         43|    Wed|     1| non_peak|         1.0|(2,[1],[1.0])|    0.0|(20,[0],[1.0])|          0.0|(2,[0],[1.0])|            0.0|(4,[0],[1.0])|\n",
      "|      CMT|         CRD|        6.0|       1.3|  7.8|        1|              1|              409|          0.7|          9|         44|    Wed|     1|  am_rush|         1.0|(2,[1],[1.0])|    0.0|(20,[0],[1.0])|          0.0|(2,[0],[1.0])|            2.0|(4,[2],[1.0])|\n",
      "|      CMT|         CSH|        7.5|       0.0|  8.0|        1|              1|              561|          0.9|         14|         44|    Wed|     0| non_peak|         1.0|(2,[1],[1.0])|    0.0|(20,[0],[1.0])|          1.0|(2,[1],[1.0])|            0.0|(4,[0],[1.0])|\n",
      "|      CMT|         CRD|        8.0|       1.0|  9.5|        1|              1|              660|          1.0|         10|         45|    Thu|     1|  am_rush|         1.0|(2,[1],[1.0])|    0.0|(20,[0],[1.0])|          0.0|(2,[0],[1.0])|            2.0|(4,[2],[1.0])|\n",
      "|      CMT|         CRD|        4.5|       1.0|  6.0|        1|              1|              248|          0.6|         14|         50|    Thu|     1| non_peak|         1.0|(2,[1],[1.0])|    0.0|(20,[0],[1.0])|          0.0|(2,[0],[1.0])|            0.0|(4,[0],[1.0])|\n",
      "|      CMT|         CSH|       11.0|       0.0| 11.5|        1|              2|              716|          2.4|          9|         51|    Wed|     0|  am_rush|         1.0|(2,[1],[1.0])|    0.0|(20,[0],[1.0])|          1.0|(2,[1],[1.0])|            2.0|(4,[2],[1.0])|\n",
      "+---------+------------+-----------+----------+-----+---------+---------------+-----------------+-------------+-----------+-----------+-------+------+---------+------------+-------------+-------+--------------+-------------+-------------+---------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoded_final.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, split `encoded_final` into `train` and `test` using 90% train, 10% test and a seed of 12345. **You must use the specified seed for reproducibility.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = encoded_final.randomSplit([0.9, 0.1], seed=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, cache the `train` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[vendor_id: string, payment_type: string, fare_amount: float, tip_amount: float, total: float, rate_code: string, passenger_count: int, trip_time_in_secs: int, trip_distance: float, pickup_hour: int, pickup_week: int, weekday: string, tipped: int, time_bins: string, vendor_index: double, vendor_vec: vector, rate_in: double, rate_vec: vector, payment_index: double, payment_vec: vector, time_bins_index: double, time_bins_vec: vector]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, cache the `test` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[vendor_id: string, payment_type: string, fare_amount: float, tip_amount: float, total: float, rate_code: string, passenger_count: int, trip_time_in_secs: int, trip_distance: float, pickup_hour: int, pickup_week: int, weekday: string, tipped: int, time_bins: string, vendor_index: double, vendor_vec: vector, rate_in: double, rate_vec: vector, payment_index: double, payment_vec: vector, time_bins_index: double, time_bins_vec: vector]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of records in the `train` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- vendor_id: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: float (nullable = true)\n",
      " |-- tip_amount: float (nullable = true)\n",
      " |-- total: float (nullable = true)\n",
      " |-- rate_code: string (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_time_in_secs: integer (nullable = true)\n",
      " |-- trip_distance: float (nullable = true)\n",
      " |-- pickup_hour: integer (nullable = true)\n",
      " |-- pickup_week: integer (nullable = true)\n",
      " |-- weekday: string (nullable = true)\n",
      " |-- tipped: integer (nullable = true)\n",
      " |-- time_bins: string (nullable = true)\n",
      " |-- vendor_index: double (nullable = false)\n",
      " |-- vendor_vec: vector (nullable = true)\n",
      " |-- rate_in: double (nullable = false)\n",
      " |-- rate_vec: vector (nullable = true)\n",
      " |-- payment_index: double (nullable = false)\n",
      " |-- payment_vec: vector (nullable = true)\n",
      " |-- time_bins_index: double (nullable = false)\n",
      " |-- time_bins_vec: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ~ 10 min - this takes time because you are actually loading the train dataset into memory as you count. \n",
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - Build a Logistic Regression Model to predict tipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this section, you will train a Logistic Regression model to predict wether or not there was a tip for each ride using the training data created in the previous section. You will build pipelines using both transformers and estimators. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, create a `LogisticRegression` estimator called `log_reg` with the following parameters: `maxIter = 10, regParam = 0.3, eslaticNetParam = 0.8`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic Regression model you will be fitting is predicting `tipped` as a function of:\n",
    "* Pickup hour\n",
    "* Passenger count\n",
    "* Trip time\n",
    "* Trip distance\n",
    "* Fare amount\n",
    "* Vendor id\n",
    "* Payment type\n",
    "* Rate code\n",
    "* Time bins\n",
    "\n",
    "Remember, you can't use the raw categorical data, you need to use something else for the categorical variables.\n",
    "\n",
    "In the next cell, create an object called `class_formula` and build a formula for the regression based on the previous instructions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_formula= VectorAssembler(inputCols= ['pickup_hour','passenger_count','trip_time_in_secs','trip_distance','fare_amount','vendor_vec',\n",
    "                'payment_vec', 'rate_vec','time_bins_vec'], outputCol= \"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, create a PipelineModel object called model, with the stages of `class_formula` and `log_reg`, in that order. Run the fit method on the `train` DataFrame. This creates a transformer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: integer (nullable = true)\n",
      " |-- pickup_hour: integer (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_time_in_secs: integer (nullable = true)\n",
      " |-- trip_distance: float (nullable = true)\n",
      " |-- fare_amount: float (nullable = true)\n",
      " |-- vendor_vec: vector (nullable = true)\n",
      " |-- payment_vec: vector (nullable = true)\n",
      " |-- rate_vec: vector (nullable = true)\n",
      " |-- time_bins_vec: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train1= train.limit(100)\n",
    "train1= train1.selectExpr(\"tipped as label\", \"pickup_hour as pickup_hour\",\"passenger_count as passenger_count\",\n",
    "                         \"trip_time_in_secs as trip_time_in_secs\", \"trip_distance as trip_distance\",\"fare_amount as fare_amount\",\n",
    "                         \"vendor_vec as vendor_vec\",\"payment_vec as payment_vec\", \"rate_vec as rate_vec\", \"time_bins_vec as time_bins_vec\"  )\n",
    "train1.printSchema()\n",
    "# train1= train1.select('pickup_hour','passenger_count','trip_time_in_secs','trip_distance','fare_amount','vendor_vec','payment_vec', 'rate_vec','time_bins_vec','label')\n",
    "model= Pipeline(stages= [class_formula, log_reg]) \n",
    "m1= model.fit(train1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, create a `predictions` DataFrame by taking the model produced in the previous cell and running the `transform` method on the `test` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pickup_hour: integer (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_time_in_secs: integer (nullable = true)\n",
      " |-- trip_distance: float (nullable = true)\n",
      " |-- fare_amount: float (nullable = true)\n",
      " |-- vendor_vec: vector (nullable = true)\n",
      " |-- payment_vec: vector (nullable = true)\n",
      " |-- rate_vec: vector (nullable = true)\n",
      " |-- time_bins_vec: vector (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- pickup_hour: integer (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_time_in_secs: integer (nullable = true)\n",
      " |-- trip_distance: float (nullable = true)\n",
      " |-- fare_amount: float (nullable = true)\n",
      " |-- vendor_vec: vector (nullable = true)\n",
      " |-- payment_vec: vector (nullable = true)\n",
      " |-- rate_vec: vector (nullable = true)\n",
      " |-- time_bins_vec: vector (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = test.limit(50)\n",
    "test= test.selectExpr(\"pickup_hour as pickup_hour\",\"passenger_count as passenger_count\",\n",
    "                      \"trip_time_in_secs as trip_time_in_secs\", \"trip_distance as trip_distance\",\n",
    "                      \"fare_amount as fare_amount\",\"vendor_vec as vendor_vec\",\"payment_vec as payment_vec\", \n",
    "                      \"rate_vec as rate_vec\", \"time_bins_vec as time_bins_vec\" ,\"tipped as label\")\n",
    "test.printSchema() \n",
    "predictions= m1.transform(test)\n",
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, create an object called `predictions_and_labels` which takes the `label` and `prediction` columns from the `predictions` DataFrame and converts it to an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions= predictions.withColumn(\"pred\", predictions.prediction.cast('float'))\\\n",
    "                        .withColumn(\"lab\", predictions.label.cast('float'))\n",
    "pred_rd= predictions.select(\"pred\", \"lab\")\n",
    "\n",
    "def getpredictions_and_labels(x):\n",
    "    \n",
    "    pred_rd= x.asDict()\n",
    "    ret_tuple= (pred_rd['pred'],pred_rd['lab'])\n",
    "    return ret_tuple\n",
    "\n",
    "predictions_and_labels= predictions.rdd.map(getpredictions_and_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, create an evaluator called metrics, and use the `BinaryClassificationMetrics` evaluator on the `predictions_and_labels` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics= BinaryClassificationMetrics(predictions_and_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this last step, the code is there for you to run. The next cell calculates the Area Under the Curve for the ROC curve generated by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC = 0.5\n"
     ]
    }
   ],
   "source": [
    "# ~13 min\n",
    "print(\"Area under ROC = %s\" % metrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think of the result of the Area under ROC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## type answer here. Do not evaluate this cell.\n",
    "Each point on the ROC curve represents a sensitivity/specificity pair corresponding to a particular decision threshold.\n",
    "The area under the ROC curve ( AUC ) is a measure of how well a parameter can distinguish between two diagnostic groups\n",
    "(diseased/normal).\n",
    "An area ROC equal to 0.5 (i.e. coinciding with the diagonal) indicates a random classification model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit (2 points)\n",
    "\n",
    "For extra credit, take the `label` and `probability` columns from the `predictions` DataFrame and create a Pandas DataFrame (local to the master node), and plot a ROC curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, make a local Pandas DataFrame from `predictions`. **Hint:** you may want to randomly sample about 20% of the results, not use the entire `predictions`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, use `matplotlib` and `scikit-learn` to plot the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (For Reference) Load a saved pipeline model and evaluate it on a data set\n",
    "\n",
    "If you did save a model as suggested in the beginning of the workbook, you can load it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "saved_model = PipelineModel.load(\"s3://[[your-s3-bucket]]/model_location/\")\n",
    "test = spark.read.parquet(\"s3://[[your-s3-bucket]]/data_location/\")\n",
    "\n",
    "predictions = saved_model.transform(test)\n",
    "prediction_and_labels = predictions.select(\"label\",\"prediction\").rdd\n",
    "\n",
    "metrics = BinaryClassificationMetrics(prediction_and_labels)\n",
    "print(\"Area under ROC = %s\" % metrics.areaUnderROC)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
